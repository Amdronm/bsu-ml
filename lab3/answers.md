## Part 1

### 1. Preprocessing
1. NAIVE BAYES (Наивный Байес):
   - Предположение: ПРИЗНАКИ НЕЗАВИСИМЫ.
   - Вердикт: НАРУШЕНО.
     Посмотрите на графики Area и ConvexArea (и Perimeter). 
     Они выглядят почти идентично. Это значит, признаки сильно коррелируют. 
     Байес будет работать хуже теоретического максимума.

2. LDA (Linear Discriminant Analysis):
   - Предположение: ОДИНАКОВЫЕ КОВАРИАЦИОННЫЕ МАТРИЦЫ (ширина колокола).
   - Вердикт: СКОРЕЕ ВСЕГО НАРУШЕНО.
     На трансформированных графиках (например, Perimeter) видно, 
     что у одного класса (фиолетовый) колокол выше и уже, 
     а у другого (желтый) — ниже и шире. Разные дисперсии мешают LDA.

3. QDA (Quadratic Discriminant Analysis):
   - Предположение: НОРМАЛЬНОЕ РАСПРЕДЕЛЕНИЕ (дисперсии могут быть разными).
   - Вердикт: ПОДХОДИТ ЛУЧШЕ ВСЕГО.
     Логарифмирование убрало хвосты, сделав распределения похожими на Гауссовы.
     Так как QDA допускает разные дисперсии классов, он должен сработать лучше LDA.




### 2. Calibration plots
Вас попросили объяснить, что мы видим. Вот шпаргалка:
Что это за график (Calibration Curve)?

Этот график показывает, насколько честна модель в своих оценках уверенности.

    Ось X: Средняя предсказанная вероятность (уверенность модели).

    Ось Y: Реальная доля положительных объектов.

    Идеал (пунктирная линия):

            
    y=xy=x

          

    . Если модель говорит "я уверена на 80%", то среди таких объектов реально должно быть 80% изюма сорта Kecimen.

Как получаются точки?

Алгоритм рисования (параметр n_bins=10):

    Все предсказания на тесте сортируются по вероятности (от 0 до 1).

    Разбиваются на 10 корзин (бинов): те, где модель дала 0-10%, 10-20% ... 90-100%.

    Для каждой корзины ставится точка:

        Координата по X: среднее предсказание модели в этой корзине.

        Координата по Y: какой процент объектов в этой корзине на самом деле оказался положительным классом.

Анализ увиденного (что скорее всего будет у вас):

    Наивный Байес (Naive Bayes) — Сигмоида (S-образная кривая)

        Что увидите: Точки лежат ниже диагонали слева и выше справа. Или вообще скапливаются у 0 и 1.

        Почему: Из-за нарушения предположения о независимости признаков (Area дублируется с ConvexArea) Байес становится сверхуверенным. Он слишком смело ставит вероятности близкие к 0 или 1, хотя реальность не такая однозначная. Он "перекалибровывает" себя.

    LDA и QDA

        Что увидите: Они обычно ближе к диагонали, чем Байес.

        Однако, если QDA переобучился (особенно при малом reg_param), он тоже может быть слишком категоричным. Но в целом, линейные дискриминанты часто дают неплохую калибровку "из коробки" на таких данных.

    Гистограммы снизу

        Показывают, как часто модель выдает ту или иную вероятность.

        У Наивного Байеса вы, скорее всего, увидите пустую середину — он почти всегда говорит либо "точно нет" (0), либо "точно да" (1). Это признак плохой калибровки, если accuracy при этом не 100%.


### 3. Recalibration
CV (Cross-Validation) в CalibratedClassifierCV означает, что ваши данные из X_train будут автоматически разбиты на несколько частей (фолдов).

    Как это работает: Если cv=5, данные бьются на 5 частей. Модель обучается на 4 частях, а на 5-й части (которую она не видела) настраивается калибратор (учится превращать "кривые" уверенности модели в реальные вероятности). Эту процедуру повторяют 5 раз.

    Последствия:

        Потеря данных для обучения: Базовый классификатор видит не 100% трейна, а меньше (например, 80%). Из-за этого Accuracy может немного упасть.

        Ансамблирование: Итоговая модель — это на самом деле усреднение 5 моделей (по одной на каждый фолд). Это делает предсказания устойчивее.

        Про ensemble=False: В старых версиях sklearn этот параметр заставлял брать только одну модель из фолда. В новых версиях (sklearn 1.0+) этот параметр удален, так как он давал нестабильные результаты. В коде ниже я не буду его использовать, чтобы код не упал с ошибкой, так как у вас Python 3.13 (а значит, свежий sklearn).


## Part 2

    Что не так с TotalCharges: Он содержал пробелы вместо чисел, поэтому Pandas считал его текстом (object).

    Пропуски: Изначально казалось, что их нет (non-null). После преобразования TotalCharges в числа (pd.to_numeric) мы увидим, что появится 11 пропусков (NaN). Это те самые строки с пробелами. Мы заполнили их нулями.

    Категориальные vs Числовые:

        Числовые: tenure, MonthlyCharges, TotalCharges (теперь это float).

        Категориальные: Все остальные (gender, Partner, InternetService, Contract и т.д.). Даже SeniorCitizen по сути категориальный (бинарный), хотя записан числами.